# -*- coding: utf-8 -*-
"""AnalyzeSentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gEQQb_iH6KAS5rDFShgCbBcuHtzZIxBk

#Instalando as bibliotecas necessárias
"""

# Instale as bibliotecas necessárias
!pip install transformers
!pip install tensorflow
!pip install sklearn
!pip install seaborn
!pip install matplotlib
!pip install torch
!pip install accelerate -U
!pip install transformers[torch]

"""#Importando as Bibliotecas"""

# Importe as bibliotecas necessárias
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from transformers import BertTokenizer, BertForSequenceClassification
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import torch
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.preprocessing import LabelEncoder

"""#Carregando o Conjunto de dados Reduzidos da IMDB
#Convertendo as Sequências de inteiros de volta para palavras
"""

# Carregue o conjunto de dados IMDB com um número reduzido de exemplos
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000, skip_top=50, maxlen=100, start_char=1, oov_char=2, index_from=3)

# Sequências de inteiros para palavras
word_index = imdb.get_word_index()
word_index = {k: (v + 3) for k, v in word_index.items()}  # Adicione 3 para compensar os tokens especiais
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2
word_index["<UNUSED>"] = 3
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

train_data = [' '.join([reverse_word_index.get(i, '<UNK>') for i in lst]) for lst in train_data]
test_data = [' '.join([reverse_word_index.get(i, '<UNK>') for i in lst]) for lst in test_data]

"""#Carregando o modelo BERT"""

# Carregue o modelo BERT
bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased') #BertBaseUncased é um modelo reduzido do BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Verifique as palavras desconhecidas no vocabulário do BERT
unknown_words = set(word for seq in train_data for word in seq.split() if word not in tokenizer.get_vocab())
print("Palavras desconhecidas no vocabulário do BERT:", unknown_words)

# Remova sequências vazias
train_data = [seq if seq.strip() != '' else '<UNK>' for seq in train_data]
test_data = [seq if seq.strip() != '' else '<UNK>' for seq in test_data]

"""#Tokenizando e codificando os dados usando o tokenizer BERT"""

# Tokenize e codifique os dados usando o tokenizer BERT
encoded_train = tokenizer.batch_encode_plus(train_data, padding=True, truncation=True, max_length=128, return_tensors='pt')
encoded_test = tokenizer.batch_encode_plus(test_data, padding=True, truncation=True, max_length=128, return_tensors='pt')

input_ids_train = encoded_train['input_ids']
token_type_ids_train = encoded_train['token_type_ids']
attention_mask_train = encoded_train['attention_mask']

input_ids_test = encoded_test['input_ids']
token_type_ids_test = encoded_test['token_type_ids']
attention_mask_test = encoded_test['attention_mask']

# Converta as listas para arrays numpy
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

# Printando algumas análises do conjunto de dados IMDB
for i in range(5):  # Imprime as primeiras 5 análises para exemplo
    print(f"Análise {i + 1}:")
    print(f"Texto: {train_data[i]}")
    print(f"Rótulo: {train_labels[i]}")
    print()

# Crie um TensorDataset e DataLoader para treino
train_dataset = TensorDataset(input_ids_train, attention_mask_train, torch.from_numpy(train_labels))
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Crie um TensorDataset e DataLoader para teste
test_dataset = TensorDataset(input_ids_test, attention_mask_test, torch.from_numpy(test_labels))
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Defina a função para avaliar o modelo
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for input_ids, attention_mask, targets in dataloader:
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    return np.array(all_preds), np.array(all_targets)

# Avaliação do modelo no conjunto de teste
eval_preds, eval_targets = evaluate_model(bert_model, test_dataloader)

# Crie um otimizador e um scheduler
optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

# Defina o número de épocas
num_epochs = 2

# Defininindo a função de perda
loss_fn = torch.nn.CrossEntropyLoss() #Criação da classe 'CrossEntropyLoss' para calcular a perda durante o treinamento.

# Início do Loop de treinamento que itera sobre cada epoch
for epoch in range(num_epochs):
    bert_model.train() # Indicando que o modelo bert está em modo de train(treinamento).
    for input_ids, attention_mask, targets in train_dataloader: # Função de entrada pro modelo
        optimizer.zero_grad() # Utilizado para zerar os gradientes acumulados dos parâmetros do modelo
        outputs = bert_model(input_ids, attention_mask=attention_mask) # Saída do modelo
        logits = outputs.logits # extração de pontuação antes da aplicação
        loss = loss_fn(logits, targets)# Calculo de perda comparando os preditos('logits') com as classes reais ('targets)
        loss.backward() # é calculado os gradientes em função do modelo
        optimizer.step() # Usado para atualizar os parâmetros com base nos gradientes.

    scheduler.step()  # Atualize o scheduler a cada época

# Avaliação do modelo no conjunto de teste
eval_preds, eval_targets = evaluate_model(bert_model, test_dataloader)

# Métricas de avaliação
accuracy = accuracy_score(eval_targets, eval_preds)
precision = precision_score(eval_targets, eval_preds, average='weighted')
recall = recall_score(eval_targets, eval_preds, average='weighted')
f1 = f1_score(eval_targets, eval_preds, average='weighted')

print(f'Acurácia: {accuracy}')
print(f'Precisão: {precision}')
print(f'Recall: {recall}')
print(f'Escore F1: {f1}')

# Matriz de Confusão
conf_matrix = confusion_matrix(eval_targets, eval_preds)

# Visualização da Matriz de Confusão
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matriz de Confusão')
plt.xlabel('Predito')
plt.ylabel('Real')
plt.show()

# Curva ROC e AUC (somente para classificação binária)
if len(np.unique(eval_targets)) == 2:
    from sklearn.metrics import roc_curve, auc

    # Calcula a probabilidade da classe positiva
    probs = bert_model(encoded_test['input_ids'], attention_mask=encoded_test['attention_mask']).logits.softmax(dim=1)[:, 1]

    # Calcula a curva ROC
    fpr, tpr, thresholds = roc_curve(eval_targets, probs)

    # Calcula a área sob a curva (AUC)
    auc_value = auc(fpr, tpr)

    # Visualização da Curva ROC
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {auc_value:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Taxa de Falso Positivo (FPR)')
    plt.ylabel('Taxa de Verdadeiro Positivo (TPR)')
    plt.title('Curva ROC')
    plt.legend(loc="lower right")
    plt.show()

# Análise de Erro
errors = np.where(eval_preds != eval_targets)[0]

# Imprima algumas predições erradas para análise
print("\nAlgumas predições erradas para análise:")
for i in range(min(5, len(errors))):
    index = errors[i]
    print(f"Texto Real: {test_data[index]}")
    print(f"Rótulo Real: {test_labels[index]}, Rótulo Predito: {eval_preds[index]}")
    print("---")